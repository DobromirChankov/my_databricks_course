{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214a178d-4ad0-49ff-95da-33531fc30c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838f2bc9-fc33-4881-bdc9-a657e1741d63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregation\n",
    "\n",
    "##### Objectives\n",
    "1. Group data by specified columns\n",
    "1. Apply grouped data methods to aggregate data\n",
    "1. Apply built-in functions to aggregate data\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`groupBy`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html\" target=\"_blank\" target=\"_blank\">Grouped Data</a>: **`agg`**, **`avg`**, **`count`**, **`max`**, **`sum`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\" target=\"_blank\">Built-In Functions</a>: **`approx_count_distinct`**, **`avg`**, **`sum`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a7dfbbb-925c-407e-a501-58bc859eda36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-00.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73667840-1058-446b-8704-db460258f71b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use the BedBricks events dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9104b9c0-497e-42ee-95b1-a49aa6b8f611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"events\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d457f9ad-a4ff-4ce2-9444-75af687b41f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Grouping data\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb1532c-6cb8-4836-82b6-fd67cae01254",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### groupBy\n",
    "Use the DataFrame **`groupBy`** method to create a grouped data object. \n",
    "\n",
    "This grouped data object is called **`RelationalGroupedDataset`** in Scala and **`GroupedData`** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba5cb24-4013-4a34-a34a-7ca9070c3736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"event_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54582a14-6d38-437c-af0c-feb8a4c43b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"geo.state\", \"geo.city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ebf8de3-c8af-45f8-b1f8-358421dcaa8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Grouped data methods\n",
    "Various aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html\" target=\"_blank\">GroupedData</a> object.\n",
    "\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| agg | Compute aggregates by specifying a series of aggregate columns |\n",
    "| avg | Compute the mean value for each numeric columns for each group |\n",
    "| count | Count the number of rows for each group |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| min | Compute the min value for each numeric column for each group |\n",
    "| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n",
    "| sum | Compute the sum for each numeric columns for each group |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17770ed7-c6c5-4d08-87dc-3a0d8faeff74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_counts_df = df.groupBy(\"event_name\").count()\n",
    "display(event_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c560dd-7673-4e73-9ae6-886398b8a0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we're getting the average purchase revenue for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0732f1d6-61f7-4a96-99a7-53d7de4854a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_state_purchases_df = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\n",
    "display(avg_state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d0b823-74fe-4240-8bb9-6737864a1a87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And here the total quantity and sum of the purchase revenue for each combination of state and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e6b8704-0c65-48df-9dd9-d5ea42fdbf66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "city_purchase_quantities_df = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\n",
    "display(city_purchase_quantities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0f1291-0c60-4bcb-97b0-a5be975da526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Built-In Functions\n",
    "In addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module.\n",
    "\n",
    "In Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_blank\">**`org.apache.spark.sql.functions`**</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">**`pyspark.sql.functions`**</a> in Python. Functions from this module must be imported into your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f0c238-b367-4f79-92ba-7741fd6dcdbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aggregate Functions\n",
    "\n",
    "Here are some of the built-in functions available for aggregation.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| approx_count_distinct | Returns the approximate number of distinct items in a group |\n",
    "| avg | Returns the average of the values in a group |\n",
    "| collect_list | Returns a list of objects with duplicates |\n",
    "| corr | Returns the Pearson Correlation Coefficient for two columns |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| stddev_samp | Returns the sample standard deviation of the expression in a group |\n",
    "| sumDistinct | Returns the sum of distinct values in the expression |\n",
    "| var_pop | Returns the population variance of the values in a group |\n",
    "\n",
    "Use the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">**`agg`**</a> to apply built-in aggregate functions\n",
    "\n",
    "This allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">**`alias`**</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5324d50-9239-4d2a-b481-abf9ebf0d5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "state_purchases_df = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\n",
    "display(state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1fdd8f6-60d2-4360-9a7e-f0872abd36b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apply multiple aggregate functions on grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3bc5f52-e191-4e46-a7be-13f122d46ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct\n",
    "\n",
    "state_aggregates_df = (df\n",
    "                       .groupBy(\"geo.state\")\n",
    "                       .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "                            approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "                      )\n",
    "\n",
    "display(state_aggregates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52d6f260-da6d-4168-8649-4120b43428a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Math Functions\n",
    "Here are some of the built-in functions for math operations.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| ceil | Computes the ceiling of the given column. |\n",
    "| cos | Computes the cosine of the given value. |\n",
    "| log | Computes the natural logarithm of the given value. |\n",
    "| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n",
    "| sqrt | Computes the square root of the specified float value. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684b0f38-9bbe-4242-a70e-af837a53ba6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cos, sqrt\n",
    "\n",
    "display(spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n",
    "        .withColumn(\"sqrt\", sqrt(\"id\"))\n",
    "        .withColumn(\"cos\", cos(\"id\"))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65dc45e9-a331-41c3-87bb-a0bf35707c76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6b8e0a-ef47-4f76-a353-f28585d0254e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377554b9-971d-4a87-b533-e3889a2018e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 0.5 - Aggregation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
