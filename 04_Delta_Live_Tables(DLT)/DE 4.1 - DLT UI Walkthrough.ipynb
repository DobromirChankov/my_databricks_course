{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fe4a55-2415-4847-9523-731fd9a94d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8acc12-e72f-4e8c-9196-b6b7dccd08c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Using the Delta Live Tables UI\n",
    "\n",
    "This demo will explore the DLT UI. By the end of this lesson you will be able to: \n",
    "\n",
    "* Deploy a DLT pipeline\n",
    "* Explore the resultant DAG\n",
    "* Execute an update of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2330b9f1-a6c3-4a43-8336-755ce34a73bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e22ee74-9e5a-465d-be15-3307547f9c96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment...\n...dropping the schema \"dnchankov_bezt_dbacademy_delp_pipeline_demo\"...(1 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\"\n\nValidating the locally installed datasets...(2 seconds)\nCreating & using the schema \"dnchankov_bezt_dbacademy_delp_pipeline_demo\"...(0 seconds)\nLoading batch 1 of 31...1 seconds\nPredefined tables in \"dnchankov_bezt_dbacademy_delp_pipeline_demo\":\n  -none-\n\nPredefined paths variables:\n  DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo\n  DA.paths.user_db:          dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/database.db\n  DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\n  DA.paths.storage_location: dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/storage_location\n  DA.paths.stream_source:    dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/stream-source\n\nSetup completed in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-04.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccf899c-1e54-404c-ae65-935a4ff5f7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate Pipeline Configuration\n",
    "The configuration of your pipeline includes parameters unique to a given user.\n",
    "\n",
    "You will need to specify which language to use by uncommenting the appropriate line.\n",
    "\n",
    "Run the following cell to print out the values used to configure your pipeline in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f244deeb-2d86-44c8-bd5a-05454dc8b9a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
       "        <td><input type=\"text\" value=\"dnchankov-bezt-dbacademy-delp-pipeline_demo: Example Pipeline\" style=\"width: 600px\"></td></tr>\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
       "        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/stream-source\" style=\"width: 600px\"></td></tr>\n",
       "\n",
       "        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
       "        <td><input type=\"text\" value=\"dnchankov_bezt_dbacademy_delp_pipeline_demo\" style=\"width: 600px\"></td></tr>\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
       "        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/storage_location\" style=\"width: 600px\"></td></tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td style=\"white-space:nowrap; width:1em\">Notebook #1 Path:</td>\n",
       "            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.1 - Orders Pipeline\" style=\"width: 600px\"></td></tr>\n",
       "        <tr>\n",
       "            <td style=\"white-space:nowrap; width:1em\">Notebook #2 Path:</td>\n",
       "            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.2 - Customers Pipeline\" style=\"width: 600px\"></td></tr>\n",
       "        <tr>\n",
       "            <td style=\"white-space:nowrap; width:1em\">Notebook #3 Path:</td>\n",
       "            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.3 - Status Pipeline\" style=\"width: 600px\"></td></tr></table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"dnchankov-bezt-dbacademy-delp-pipeline_demo: Example Pipeline\" style=\"width: 600px\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/stream-source\" style=\"width: 600px\"></td></tr>\n\n        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n        <td><input type=\"text\" value=\"dnchankov_bezt_dbacademy_delp_pipeline_demo\" style=\"width: 600px\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/dnchankov@abv.bg/data-engineer-learning-path/pipeline_demo/storage_location\" style=\"width: 600px\"></td></tr>\n    \n        <tr>\n            <td style=\"white-space:nowrap; width:1em\">Notebook #1 Path:</td>\n            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.1 - Orders Pipeline\" style=\"width: 600px\"></td></tr>\n        <tr>\n            <td style=\"white-space:nowrap; width:1em\">Notebook #2 Path:</td>\n            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.2 - Customers Pipeline\" style=\"width: 600px\"></td></tr>\n        <tr>\n            <td style=\"white-space:nowrap; width:1em\">Notebook #3 Path:</td>\n            <td><input type=\"text\" value=\"/data-engineer-learning-path-v1.0.1-notebooks/04 - Delta Live Tables/DE 4.1A - SQL Pipelines/DE 4.1.3 - Status Pipeline\" style=\"width: 600px\"></td></tr></table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_language = \"SQL\"\n",
    "# pipeline_language = \"Python\"\n",
    "\n",
    "DA.print_pipeline_config(pipeline_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a654aa7-b548-4349-863f-0603c193ac8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To run the cell when we're so far\n",
    "# pipeline_language = \"Python\" \n",
    "DA.validate_pipeline_config(pipeline_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e0eeb6-4414-42ca-86f4-d67295da9dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"> **HINT:** You will want to refer back to the paths above for Notebook #2 and Notebook #3 in later lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b9e71c-ae82-4aa8-ae71-94ffb5a6ffa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create and configure a pipeline\n",
    "\n",
    "In this section you will create a pipeline using a single notebook provided with the courseware.\n",
    "\n",
    "We'll explore the contents of this notebook in the following lesson.\n",
    "\n",
    "We will later add Notebooks #2 & #3 to the pipeline but for now, let's focus on just Notebook #1:\n",
    "\n",
    "1. Click the **Workflows** button in the sidebar\n",
    "1. Select the **Delta Live Tables** tab.\n",
    "1. Click the **Create Pipeline** button.\n",
    "1. In the field **Product edition**, select the value \"**Advanced**\".\n",
    "1. In the field **Pipeline Name**, enter the value specified in the cell above\n",
    "1. In the field **Notebook Libraries**, copy the path for Notebook #1 specified in the cell above and paste it here.\n",
    "    * Though this document is a standard Databricks Notebook, the syntax is specialized to DLT table declarations.\n",
    "    * We will be exploring the syntax in the exercise that follows.\n",
    "    * Notebooks #2 and #3 will be added in later lessons\n",
    "1. Configure the **Source**\n",
    "    1. Click the **Add configuration** button\n",
    "    1. In the field **Key**, enter the word \"**source**\"\n",
    "    1. In the field **Value**, enter the **Source** value specified in the cell above\n",
    "1. In the field **Storage location**, enter the value specified in the cell above.\n",
    "    * This optional field allows the user to specify a location to store logs, tables, and other information related to pipeline execution. If not specified, DLT will automatically generate a directory.\n",
    "1. Set **Pipeline Mode** to **Triggered**.\n",
    "    * This field specifies how the pipeline will be run.\n",
    "    * **Triggered** pipelines run once and then shut down until the next manual or scheduled update.\n",
    "    * **Continuous** pipelines run continuously, ingesting new data as it arrives.\n",
    "    * Choose the mode based on latency and cost requirements.\n",
    "1. Disable autoscaling by unchecking **Enable autoscaling**.\n",
    "    * **Enable autoscaling**, **Min Workers** and **Max Workers** control the worker configuration for the underlying cluster processing the pipeline.\n",
    "    * Notice the DBU estimate provided, similar to that provided when configuring interactive clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18460760-11f4-4ef0-979e-67e124d81b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Running in Local-Mode    \n",
    "* We need to configure the pipeline to run on a local-mode cluster.\n",
    "* In most deployments, we would adjust the number of workers to accomodate the scale of the pipeline.\n",
    "* Our datasets are really small, we are just prototyping, so we can use a Local-Mode cluster which reduces cloud costs by employing only a single VM.\n",
    "\n",
    "Local-Mode Setup:    \n",
    "1. In the fields **Workers**, set the value to \"**0**\" (zero) - worker and driver will use the same VM.\n",
    "1. Click **Add configuration** and then set the key to **spark.master** and the corresponding value to **local[*]**\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **WARNING:** Setting works to zero and failing to configure **spark.master** will result in a failure to create your cluster as it will be waiting forever for a worker that will never be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa163ab-9896-447a-9477-b27f2b48b082",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Final Steps\n",
    "1. Click the **Create** button\n",
    "1. Verify that the pipeline mode is set to \"**Development**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4116bdc1-080d-40c0-8bad-e62403fa264c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run a pipeline\n",
    "\n",
    "With a pipeline created, you will now run the pipeline.\n",
    "\n",
    "1. Select **Development** to run the pipeline in development mode. Development mode provides for more expeditious iterative development by reusing the cluster (as opposed to creating a new cluster for each run) and disabling retries so that you can readily identify and fix errors. Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#optimize-execution\" target=\"_blank\">documentation</a> for more information on this feature.\n",
    "2. Click **Start**.\n",
    "\n",
    "The initial run will take several minutes while a cluster is provisioned. Subsequent runs will be appreciably quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1c33e8-28a8-413a-9366-b612680f329e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploring the DAG\n",
    "\n",
    "As the pipeline completes, the execution flow is graphed. \n",
    "\n",
    "Selecting the tables reviews the details.\n",
    "\n",
    "Select **orders_silver**. Notice the results reported in the **Data Quality** section. \n",
    "\n",
    "With each triggered update, all newly arriving data will be processed through your pipeline. Metrics will always be reported for current run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231396bc-7aaf-45e9-afba-d8cac39cf866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Land another batch of data\n",
    "\n",
    "Run the cell below to land more data in the source directory, then manually trigger a pipeline update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fb7f5b-5222-45e6-b0c6-57d4a677b3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch 2 of 31...1 seconds\nOut[14]: True"
     ]
    }
   ],
   "source": [
    "DA.dlt_data_factory.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da3a48e-bdf2-46ba-932c-4cfe922e4e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we continue through the course, you can return to this notebook and use the method provided above to land new data.\n",
    "\n",
    "Running this entire notebook again will delete the underlying data files for both the source data and your DLT Pipeline. \n",
    "\n",
    "If you get disconnected from your cluster or have some other event where you wish to land(получаваш) more data without deleting things, refer to the <a href=\"$./DE 4.99 - Land New Data\" target=\"_blank\">DE 4.99 - Land New Data</a> notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb14da3-c873-49a5-a1d5-1560bce9ffc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 4.1 - DLT UI Walkthrough",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
