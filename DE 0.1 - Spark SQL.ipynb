{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01acabcc-160b-457a-baf4-368fccfebcba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "564de9e9-4245-4274-b5d1-9422f7677e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "Demonstrate fundamental concepts in Spark SQL using the DataFrame API.\n",
    "\n",
    "##### Objectives\n",
    "1. Run a SQL query\n",
    "1. Create a DataFrame from a table\n",
    "1. Write the same query using DataFrame transformations\n",
    "1. Trigger computation with DataFrame actions\n",
    "1. Convert between DataFrames and SQL\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html\" target=\"_blank\">SparkSession</a>: **`sql`**, **`table`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>:\n",
    "  - Transformations:  **`select`**, **`where`**, **`orderBy`**\n",
    "  - Actions: **`show`**, **`count`**, **`take`**\n",
    "  - Other methods: **`printSchema`**, **`schema`**, **`createOrReplaceTempView`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba8538e-2476-41cf-8390-87b6ba4aa665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-00.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d1a4626-5ba7-4df1-9304-04539db68746",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Multiple Interfaces\n",
    "Spark SQL is a module for structured data processing with multiple interfaces.\n",
    "\n",
    "We can interact with Spark SQL in two ways:\n",
    "1. Executing SQL queries\n",
    "1. Working with the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e1b687-9f6c-42cf-88f1-cd5ad766ece2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Method 1: Executing SQL queries**\n",
    "\n",
    "This is a basic SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34615b49-a3a9-4fde-a191-4e5d8820cf47",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT name, price\n",
    "FROM products\n",
    "WHERE price < 200\n",
    "ORDER BY price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1504e179-259e-4a9b-ad4e-c1c3f34aa75f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Method 2: Working with the DataFrame API**\n",
    "\n",
    "We can also express Spark SQL queries using the DataFrame API.\n",
    "The following cell returns a DataFrame containing the same results as those retrieved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c09ea78-3a3b-4a90-8d3c-88f268a572be",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "display(spark\n",
    "        .table(\"products\")\n",
    "        .select(\"name\", \"price\")\n",
    "        .where(\"price < 200\")\n",
    "        .orderBy(\"price\")\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5abbf843-328e-4b68-9990-d79a51890458",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'll go over the syntax for the DataFrame API later in the lesson, but you can see this builder design pattern allows us to chain a sequence of operations very similar to those we find in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ae981fc-7d25-4def-ba3c-73f42801203f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query Execution\n",
    "We can express the same query using any interface. The Spark SQL engine generates the same query plan used to optimize and execute on our Spark cluster.\n",
    "\n",
    "![query execution engine](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Resilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\">code manipulating RDDs directly</a>. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e5c6a8-4942-4243-9150-e537355c8606",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark API Documentation\n",
    "\n",
    "To learn how we work with DataFrames in Spark SQL, let's first look at the Spark API documentation.\n",
    "The main Spark <a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">documentation</a> page includes links to API docs and helpful guides for each version of Spark.\n",
    "\n",
    "The <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html\" target=\"_blank\">Scala API</a> and <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">Python API</a> are most commonly used, and it's often helpful to reference the documentation for both languages.\n",
    "Scala docs tend to be more comprehensive, and Python docs tend to have more code examples.\n",
    "\n",
    "#### Navigating Docs for the Spark SQL Module\n",
    "Find the Spark SQL module by navigating to **`org.apache.spark.sql`** in the Scala API or **`pyspark.sql`** in the Python API.\n",
    "The first class we'll explore in this module is the **`SparkSession`** class. You can find this by entering \"SparkSession\" in the search bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2bee446-f8b1-4ec1-af34-d1eef59a59f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SparkSession\n",
    "The **`SparkSession`** class is the single entry point to all functionality in Spark using the DataFrame API.\n",
    "\n",
    "In Databricks notebooks, the SparkSession is created for you, stored in a variable called **`spark`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26837c27-a048-429b-be27-2412059d6da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116733c9-7926-43b0-8f52-d8a8ed939f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The example from the beginning of this lesson used the SparkSession method **`table`** to create a DataFrame from the **`products`** table. Let's save this in the variable **`products_df`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e25d3f-5980-4790-a4c5-3c492ecab15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df = spark.table(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d822125e-f407-4df7-a44b-c203f5ffbf4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are several additional methods we can use to create DataFrames. All of these can be found in the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html\" target=\"_blank\">documentation</a> for **`SparkSession`**.\n",
    "\n",
    "#### **`SparkSession`** Methods\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| sql | Returns a DataFrame representing the result of the given query |\n",
    "| table | Returns the specified table as a DataFrame |\n",
    "| read | Returns a DataFrameReader that can be used to read data in as a DataFrame |\n",
    "| range | Create a DataFrame with a column containing elements in a range from start to end (exclusive) with step value and number of partitions |\n",
    "| createDataFrame | Creates a DataFrame from a list of tuples, primarily used for testing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79479055-74f2-448e-a160-7fc12de4dc0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use a SparkSession method to run SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28fea01c-7557-454a-964d-b9514dd738a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "SELECT name, price\n",
    "FROM products\n",
    "WHERE price < 200\n",
    "ORDER BY price\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e943452-b737-4838-9a45-490cd7e3ebc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrames\n",
    "Recall that expressing our query using methods in the DataFrame API returns results in a DataFrame. Let's store this in the variable **`budget_df`**.\n",
    "\n",
    "A **DataFrame** is a distributed collection of data grouped into named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b85ce6f-7f06-465e-8549-ad0d24a8c75b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df = (spark\n",
    "             .table(\"products\")\n",
    "             .select(\"name\", \"price\")\n",
    "             .where(\"price < 200\")\n",
    "             .orderBy(\"price\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e80b669e-911c-4c7a-adcd-5448e11e9f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can use **`display()`** to output the results of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f224e2e-a6bb-4b23-9655-dde09f2a407d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(budget_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d6fde7-a4b7-4be1-8929-cc8c83f9f85c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The **schema** defines the column names and types of a dataframe.\n",
    "\n",
    "Access a dataframe's schema using the **`schema`** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf0ed4a1-cdcc-43f9-9c8f-34d8b42ff8c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e50e242-29b0-4434-acb9-d4c4aefda082",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "View a nicer output for this schema using the **`printSchema()`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f8bf279-b302-4823-bdf8-f940d9783b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a2a8991-f680-4522-b654-7d2b629f8028",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "When we created **`budget_df`**, we used a series of DataFrame transformation methods e.g. **`select`**, **`where`**, **`orderBy`**.\n",
    "\n",
    "<strong><code>products_df  \n",
    "&nbsp;  .select(\"name\", \"price\")  \n",
    "&nbsp;  .where(\"price < 200\")  \n",
    "&nbsp;  .orderBy(\"price\")  \n",
    "</code></strong>\n",
    "    \n",
    "Transformations operate on and return DataFrames, allowing us to chain transformation methods together to construct new DataFrames.\n",
    "However, these operations can't execute on their own, as transformation methods are **lazily evaluated**.\n",
    "\n",
    "Running the following cell does not trigger any computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db36e96-664c-4a50-9408-44895bebdd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(products_df\n",
    "  .select(\"name\", \"price\")\n",
    "  .where(\"price < 200\")\n",
    "  .orderBy(\"price\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ccf514-309f-419e-a828-b55bbe033c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Actions\n",
    "Conversely, DataFrame actions are methods that **trigger computation**.\n",
    "Actions are needed to trigger the execution of any DataFrame transformations.\n",
    "\n",
    "The **`show`** action causes the following cell to execute transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6784eb29-9f19-42a8-b4e3-5b8db7832fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(products_df\n",
    "  .select(\"name\", \"price\")\n",
    "  .where(\"price < 200\")\n",
    "  .orderBy(\"price\")\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1913a7-3915-44a9-a805-b19fe86e2c2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are several examples of <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis\" target=\"_blank\">DataFrame</a> actions.\n",
    "\n",
    "### DataFrame Action Methods\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| show | Displays the top n rows of DataFrame in a tabular form |\n",
    "| count | Returns the number of rows in the DataFrame |\n",
    "| describe,  summary | Computes basic statistics for numeric and string columns |\n",
    "| first, head | Returns the the first row |\n",
    "| collect | Returns an array that contains all rows in this DataFrame |\n",
    "| take | Returns an array of the first n rows in the DataFrame |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e30b5f-d41d-4069-947c-a014f17f4504",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**`count`** returns the number of records in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324a10bd-3097-4c61-b8ca-49e36641413a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82e532b0-f8f0-4c10-94da-a65941a42047",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**`collect`** returns an array of all rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93ec885-c0f7-4379-9850-d174a64b34f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "797b4873-c315-43e5-a784-ffc829c9b181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Convert between DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2d6ff2-41f4-4e9d-ab42-da28292b1578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**`createOrReplaceTempView`** creates a temporary view based on the DataFrame. The lifetime of the temporary view is tied to the SparkSession that was used to create the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abae48bd-1593-4041-bbeb-e2705c55f8ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "budget_df.createOrReplaceTempView(\"budget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83cfd0bc-ddc7-4cc8-a8a0-98598aea92f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM budget\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6154e52-ba98-4d74-af05-30125ba09a38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29359edc-5ee8-480e-95ca-df8b5da56330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb133415-06d5-46cf-bae0-c6dab9bb7cab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Създаване на данните\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Emma', 'Bob', 'Sophia', 'Michael', 'Olivia', 'James', 'Emily',\n",
    "             'Sophia', 'Mia', 'Jacob', 'Isabella', 'Matthew', 'Charlotte', 'Andrew', 'Abigail', 'Ryan', 'Sophia'],\n",
    "    'Age': [25, 30, 45, 28, 35, 42, 31, 27, 39, 33, 29, 36, 41, 24, 38, 26, 32, 37, 43, 34],\n",
    "    'City': ['Boston', 'Detroit', 'Chicago', 'San Francisco', 'Boston', 'Seattle', 'Detroit', 'Detroit', 'Detroit', 'Detroit',\n",
    "             'Boston', 'Miami', 'Boston', 'Phoenix', 'Portland', 'San Diego', 'Boston', 'Tampa', 'Detroit', 'Boston']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Конвертиране на DataFrame в pyarrow таблица\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Запис на таблицата в Parquet файл\n",
    "parquet_file = pq.write_table(table, 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "Name: string\n",
       "Age: int64\n",
       "City: string\n",
       "----\n",
       "Name: [[\"John\",\"Alice\",\"Bob\",\"Emma\",\"Bob\",...,\"Charlotte\",\"Andrew\",\"Abigail\",\"Ryan\",\"Sophia\"]]\n",
       "Age: [[25,30,45,28,35,...,26,32,37,43,34]]\n",
       "City: [[\"Boston\",\"Detroit\",\"Chicago\",\"San Francisco\",\"Boston\",...,\"San Diego\",\"Boston\",\"Tampa\",\"Detroit\",\"Boston\"]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(path = 'example.parquet').Age.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(path = 'example.parquet').Name.nunique()  # -> 17\n",
    "pd.read_parquet(path = 'example.parquet').City.nunique()  # -> 10\n",
    "pd.read_parquet(path = 'example.parquet').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mPySpark Example\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      5\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([(\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAlice\u001b[39m\u001b[39m\"\u001b[39m), (\u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBob\u001b[39m\u001b[39m\"\u001b[39m)], [\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dobromir\\Privat\\CV\\DXC\\Databricks\\0. Intro to PySpark\\DE 0.1 - Spark SQL.ipynb Cell 49\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dobromir/Privat/CV/DXC/Databricks/0.%20Intro%20to%20PySpark/DE%200.1%20-%20Spark%20SQL.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dobromir/Privat/CV/DXC/Databricks/0.%20Intro%20to%20PySpark/DE%200.1%20-%20Spark%20SQL.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m usersDF \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mtable(\u001b[39m\"\u001b[39m\u001b[39musers_dirty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dobromir/Privat/CV/DXC/Databricks/0.%20Intro%20to%20PySpark/DE%200.1%20-%20Spark%20SQL.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m usersDF\u001b[39m.\u001b[39mselectExpr(\u001b[39m\"\u001b[39m\u001b[39mcount_if(email IS NULL)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dobromir/Privat/CV/DXC/Databricks/0.%20Intro%20to%20PySpark/DE%200.1%20-%20Spark%20SQL.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m usersDF\u001b[39m.\u001b[39mwhere(col(\u001b[39m\"\u001b[39m\u001b[39memail\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misNull())\u001b[39m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "usersDF = spark.read.table(\"users_dirty\")\n",
    "\n",
    "usersDF.selectExpr(\"count_if(email IS NULL)\")\n",
    "usersDF.where(col(\"email\").isNull()).count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 0.1 - Spark SQL",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
